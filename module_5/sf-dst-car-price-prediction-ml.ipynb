{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Прогнозирование стоимости автомобиля по характеристикам\n\n\nОбразовательная платформа: SkillFactory\n\nСпециализация: Data Science\n\nГруппа: DST-37 и 38\n\nЮнит 6. Проект 5: \"Выбираем автомобиль правильно\"\n\n\n### Задача:\n\n    Создать модель, которая будет предсказывать стоимость автомобиля по его характеристикам для того, чтобы выявлять выгодные предложения (когда желаемая цена продавца ниже предсказанной рыночной цены).\n\n### Метрика:\n\n    MAPE (Mean Percentage Absolute Error) - средняя абсолютная ошибка в процентах\n\n### Нужно:\n\n    Составить train датасет - спарсить данные, либо найти готовый\n    Обучить модель\n\n### Плюс:\n\n    Посмотреть, что можно извлечь из признаков или как еще можно обработать признаки\n    Сгенерировать новые признаки\n    Подгрузить еще больше данных\n    Попробовать подобрать параметры модели\n    Попробовать разные алгоритмы и библиотеки ML\n    Сделать Ансамбль моделей, Blending, Stacking\n\n### Этапы работы:\n\n    Парсинг с авто.ру - Вадим, Евгений, Артём\n    EDA, Feature Engineering - Артём, Вадим, Евгений\n    Сравнение одиночных моделей - Артём, Вадим\n    Стекинг - Вадим\n    \n    \n#### В данном ноутбуке мы проводим обучение моделей и выбираем лучшую для предсказания.\n\n#### Также в этом проекте мы использовали:\n\nНоутбук, через который пытались парсить: https://www.kaggle.com/artemskakun/sf-dst-car-price-prediction-autoruparser\n\nСпарсенный датасет мы взяли у этой команды, потому что парсинг занимал очень много времени: https://www.kaggle.com/juliadeinego/data-car-sales\n\nНоутбук, в котором провели EDA: https://www.kaggle.com/artemskakun/sf-dst-car-price-prediction-datapreprocessing","metadata":{}},{"cell_type":"code","source":"# импорт библиотек\n\nimport glob\nimport pandas as pd\nimport numpy as np\nimport json\nimport csv\nfrom datetime import datetime\nfrom ast import literal_eval\nimport pandas.api.types as at\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport xgboost as xgb\n\nfrom tqdm import tqdm\nfrom sklearn.base import clone\n\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_colwidth', None)\n\nRANDOM_SEED = 42\n\nTEST_DATA = '../input/preproc/'\n# TEST_DATA = 'D:/skillfactory_rds/CarPricePrediction/'\n# TEST_DATA = 'data/test/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Функции","metadata":{}},{"cell_type":"code","source":"# функция распределения признаков по типу данных\ndef sort_features(df_raw, target_cols, time_cols, num_cols, bin_cols, cat_cols, count_col):\n    for col in df_raw.columns:\n        if col in target_cols or col in time_cols or col in num_cols\\\n                or col in bin_cols or col in cat_cols or col in count_col:\n            continue\n        elif len(df_raw[col].value_counts()) == 1:\n            df_raw.drop(columns=[col], inplace=True)\n        elif at.is_datetime64_any_dtype(df_raw[col]):\n            time_cols.append(col)\n        elif at.is_numeric_dtype(df_raw[col]):\n            if len(df_raw[col].value_counts()) == 2:\n                bin_cols.append(col)\n            else:\n                num_cols.append(col)\n        elif at.is_string_dtype(df_raw[col]):\n            cat_cols.append(col)\n        else:\n            print(\n                f'Столбец {col} не был причислен ни к одной категории\\n'+'_'*50)\n\n    print_cols_lists(df_raw, target_cols, time_cols,\n                     num_cols, bin_cols, cat_cols, count_col)\n\n    return target_cols, time_cols, num_cols, bin_cols, cat_cols, count_col\n\n# функция вывода данных по типу\ndef print_cols_lists(df, target_cols, time_cols, num_cols, bin_cols, cat_cols, count_col):\n    print('\\nКлючевые признаки: ', target_cols)\n    print('\\nПризнаки даты или времени: ', time_cols)\n    print('\\nКатегориальные признаки: ', cat_cols)\n    print('\\nБинарные признаки: ', bin_cols)\n    print('\\nКоличественные признаки: ', num_cols)\n    print('\\nПризнаки-счетчики: ', count_col)\n    print('\\nВ датасете: строк - ', len(df), 'колонок - ', len(df.columns))\n\n# рассчёт поправочного коэффициента в зависимости от изменение курса доллара\nprev_rate = 77.9241\ncurr_rate = 76.9808  # 16/04/21\nrate_coeff = prev_rate/curr_rate\nprint(f'поправочный коэффициент {rate_coeff}')\ndate_before = datetime.strptime('01/04/2021', '%d/%m/%Y')\n\n# функция обучения модели и вывода MAPE\n# is_log - передавать значение True, если y_train прологарифмирован\ndef learn_model(model, X_train, X_test, y_train, y_test, is_log=False):\n    model.fit(X_train, y_train)\n    y_pred = []\n    if is_log:\n        print('predict logarithmic')\n        y_pred = np.round(np.exp(model.predict(X_test))).astype(int)\n    else:\n        print('predict native')\n        y_pred = model.predict(X_test)\n    mape = mean_absolute_percentage_error(y_test, y_pred)\n    print(f\"Средняя абсолютная ошибка в процентах: {mape*100:0.2f}%\")\n    return mape\n\n# функция сохранения результатов в списке results\ndef safe_results(results, model_name, mape, test_prediction, sabmission):\n    results[model_name] = {\n        'mape': mape, 'test_prediction': test_prediction, 'submission': sabmission}\n\n# функция получения предсказания. функция учитывает изменение курса доллара между временем парсинга обучаещего\n# и тестового датасета через rate_coeff\ndef model_prediction(model, X_test, test_df, is_log=False):\n    predict_test = []\n    predict_submission = []\n    if is_log:\n        print('predict logarithmic')\n        predict_test = np.round(np.exp(model.predict(X_test)), -3).astype(int)\n        predict_submission = np.round(\n            np.exp(model.predict(test_df))/rate_coeff, -3).astype(int)\n    else:\n        print('predict native')\n        predict_test = np.round(model.predict(X_test), -3)\n        predict_submission = np.round(model.predict(test_df)/rate_coeff, -3)\n\n    return predict_test, predict_submission\n\n# функция сохранения прогноза в файл\ndef write_submission_to_file(name, submission):\n    submission = np.around(submission).astype(int)\n    with open(f'{TEST_DATA}{name}.csv', mode='w') as submission_file:\n        writer = csv.writer(submission_file, delimiter=',',\n                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(submission.tolist())\n\n# функция подставления прогноза\ndef make_submission(predict_submission, version):\n    test_submission = pd.read_csv(f'{TEST_DATA}sample_submission.csv')\n    predict = np.around(predict_submission).astype(int)\n    test_submission['price'] = predict\n\n    test_submission.to_csv(\n        f'{TEST_DATA}submission_v{version}.csv', index=False)\n\n# Функция для определения границ выбросов\ndef get_outliners(column):\n    koeff = 1.5\n    median = column.median()\n    quan25 = column.quantile(0.25)\n    quan75 = column.quantile(0.75)\n    IQR = quan75 - quan25\n    left = quan25 - koeff*IQR,\n    right = quan75 + koeff*IQR\n    print(f\"Границы выбросов для столбца '{column.name}': [{left}, {right}]\")\n    return(left, right)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Загружаем предподготовленные данные","metadata":{}},{"cell_type":"code","source":"full_df = pd.read_csv(f'{TEST_DATA}preproc.csv')\nfull_df.sample(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Подготовим списки признаков по типам","metadata":{}},{"cell_type":"code","source":"target_cols = ['price']\nnum_cols, bin_cols, cat_cols, time_cols, count_col = [], [], [], [], []\n\ntarget_cols,time_cols,num_cols,bin_cols,cat_cols,count_col = sort_features(full_df\n                                                                           ,target_cols,time_cols\n                                                                           ,num_cols,bin_cols\n                                                                           ,cat_cols,count_col)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Оценка степени влияния признаков на целевые переменные\n\n#### Посмотрим на распределение числовых данных","metadata":{}},{"cell_type":"code","source":"for i in num_cols:\n    plt.figure()\n    sns.distplot(full_df[i][full_df[i] > 0].dropna(), kde = False, rug=False)\n    plt.title(i)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Распределение признаков нормальное.\n\nХотя количество владельцев и выглядит как категоральный признак, оставляем его числовым, т.к. количество владельцев явно влияет на цену автомобиля.\n\nКоличество дверей перенесём в категориальные признаки.","metadata":{}},{"cell_type":"code","source":"num_cols.remove('number_of_doors')\ncat_cols.append('number_of_doors')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Попробуем прологарифмировать колонки и проверим распределение на копии датафрейма","metadata":{}},{"cell_type":"code","source":"full_df_copy = full_df.copy()\ncols = ['engine_displacement', 'engine_power']\nfig, axes = plt.subplots(1, len(cols), figsize=(10,7))\nfor i,col in enumerate(cols):\n    full_df_copy[col] = np.log(full_df_copy[col] + 1)\n    sns.distplot(full_df_copy[col][full_df_copy[col] > 0].dropna(), ax=axes.flat[i],kde = False, rug=False, color=\"b\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Попробуем взять квадратный корень тех же колонок и проверим распределение на копии датафрейма","metadata":{}},{"cell_type":"code","source":"full_df_copy = full_df.copy()\ncols = ['engine_displacement', 'engine_power']\nfig, axes = plt.subplots(1, len(cols), figsize=(10,7))\nfor i,col in enumerate(cols):\n    full_df_copy[col] = np.sqrt(full_df_copy[col])\n    sns.distplot(full_df_copy[col][full_df_copy[col] > 0].dropna(), ax=axes.flat[i],kde = False, rug=False, color=\"b\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Логарифмирование признаков улучшает распределение\n\nПрологарифмируем признаки engine_displacement и engine_power","metadata":{}},{"cell_type":"code","source":"cols = ['engine_displacement', 'engine_power']\nfor i,col in enumerate(cols):\n    full_df[col] = np.log(full_df[col] + 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Построим boxplot’ы для числовых переменных","metadata":{}},{"cell_type":"code","source":"for i in num_cols:\n    plt.figure()\n    sns.boxplot(full_df[i][full_df[i] > 0].dropna())\n    plt.title(i)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Обработка выбросов числовых признаков","metadata":{}},{"cell_type":"code","source":"left, right = get_outliners(full_df['price'])\nfull_df[((full_df['price'] < left) | (full_df['price'] > right)) & (full_df['test'] == 0)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"left, right = get_outliners(full_df['engine_power'])\nfull_df[((full_df['engine_power'] < left) | (full_df['engine_power'] > right)) & (full_df['test'] == 0)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"left, right = get_outliners(full_df['mileage'])\nfull_df[((full_df['mileage'] < left) | (full_df['mileage'] > right)) & (full_df['test'] == 0)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"left, right = get_outliners(full_df['model_date'])\nfull_df[((full_df['model_date'] < left) | (full_df['model_date'] > right)) & (full_df['test'] == 0)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Явно видны выбросы на всех признаках, но они связаны с присутствием редких и дорогих моделей. Мы не будем от них отказываться, т.к. хотелось бы, чтобы модель умела предсказывать и их стоимость.\nВыбросы по дате модели находятся в тестовой выборке, так что их тоже не будем трогать.\nТем не менее мы обучали модели на очищенной от выбросов выборке, но это не дало прироста качества моделей.","metadata":{}},{"cell_type":"markdown","source":"### Оценка корреляции числовых признаков","metadata":{}},{"cell_type":"code","source":"cols = num_cols.copy()\ncols.append('price')\nsns.heatmap(full_df[full_df['test']==0][cols].corr().abs(), vmin=0, vmax=1, annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Мы видим сильную корреляцию всех признаков с целевым. \n\nТак же присутствует сильная корреляция между датой выпуска, пробегом и количеством владельцев, что и понятно.","metadata":{}},{"cell_type":"markdown","source":"### Оценим значимость числовых признаков\n\nПосмотрим на коэффициент корреляции Пирсона между признаками и целевой переменной.","metadata":{}},{"cell_type":"code","source":"X_train = full_df[full_df['test']==0].drop('price', axis = 1)\ny_train = full_df[full_df['test']==0]['price']\ncorrelations = X_train.corrwith(y_train).sort_values(ascending=False)\nplot = sns.barplot(y=correlations.index, x=correlations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"График подтверждает реальную ситуацию: количество владельцев и пробег отрицательно влияют на стоимость автомобиля, а мощность, объем двигателя и более поздняя дата выпуска увеличивают ее.","metadata":{}},{"cell_type":"markdown","source":"#### Добавим новые признаки возведя в квадрат пробег, объем двигателя и его мощность для того, что бы усилить не линейное влияние этих признаков при обучении модели.","metadata":{}},{"cell_type":"code","source":"for col in ['engine_displacement','engine_power','mileage']:\n    name = col + '_x2'\n    full_df[name] = full_df[col].pow(2)\n    num_cols.append(name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"еще раз оценим корреляцию","metadata":{}},{"cell_type":"code","source":"cols = num_cols.copy()\ncols.append('price')\nsns.heatmap(full_df[full_df['test']==0][cols].corr().abs(), vmin=0, vmax=1, annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.sample(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Исследуем категориальные признаки","metadata":{}},{"cell_type":"code","source":"for col in ['body_type','color','vehicle_transmission','vehicle_pasport','wheel','brand']:\n    ax = sns.countplot(x=col, data=full_df)\n    ax.xaxis.set_tick_params(rotation=45)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df[cat_cols].sample(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.model_name.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Очень большое количество значений имеет имя модели.\n\nВсвязи с большим разнообразием наименований моделей, преобразуем их.\n\n1% наименее используемый обозначим как имя бренда.","metadata":{}},{"cell_type":"code","source":"model_lst = []\nfor brand in full_df['brand'].unique():\n    count_brand = full_df[full_df['brand'] == brand]['price'].count()\n    for model in full_df[full_df['brand'] == brand]['model_name'].unique():\n        count_model = full_df[full_df['model_name'] == model]['price'].count()\n        if count_model/count_brand < 0.01:\n            model_lst.append(model)\n\nfor model in model_lst:\n    try:\n        full_df.loc[full_df['model_name'] == model, 'model_name'] = full_df[full_df['model_name'] == model]['brand'].unique()[0]\n    except:\n        pass           ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.model_name.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь количество моделей стало более приемлемым.","metadata":{}},{"cell_type":"markdown","source":"#### Для оценки значимости категориальных признаков, преобразуем их в числовые значения.\n\nДля начала используем LabelEncoder для преобразования категориальных признаков в числовые значения, предварительно скопировав в новый датафрейм.","metadata":{}},{"cell_type":"code","source":"df_copy = full_df[full_df['test']==0][cat_cols + target_cols]\n\nlabel_encoder = LabelEncoder()\n\nfor column in cat_cols:\n    df_copy[column] = label_encoder.fit_transform(df_copy[column])\n    \n# убедимся в преобразовании    \ndf_copy.sample(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Оценим значимость категориальных признаков","metadata":{}},{"cell_type":"code","source":"X_train = df_copy.drop('price', axis = 1)\ny_train = df_copy['price']\n\nimp_cat = pd.Series(mutual_info_classif(X_train, y_train,\\\n                    discrete_features = True), index = X_train.columns)\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наибольшее влияние на стоимость оказывают модель, брэнд, тип кузова, цвет и тип привода.","metadata":{}},{"cell_type":"markdown","source":"### Машинное обучение","metadata":{}},{"cell_type":"markdown","source":"#### Преобразуем категоральные признаки в отдельные признаки.","metadata":{}},{"cell_type":"code","source":"dummies = pd.get_dummies(full_df[cat_cols])\nfull_df = full_df.drop(cat_cols, axis=1).join(dummies)\nfull_df.sample(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Разделим обучающую выборку для обучения","metadata":{}},{"cell_type":"code","source":"X = full_df[full_df['test']==0].drop(['price'], axis = 1)\nX = X.drop('test', axis = 1)\ny = full_df[full_df['test']==0]['price'].values\n\ntest_df = full_df[full_df['test'] == 1].drop(['test','price'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42, test_size=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Проведем стандартизацию числовых признаков, предварительно отделив тестовую и обучающую выборку.","metadata":{}},{"cell_type":"code","source":"scaller = StandardScaler()\n\nX_train_transformed = X_train.copy()\nX_train_transformed[num_cols] = scaller.fit_transform(X_train_transformed[num_cols])\n\nX_test_transformed = X_test.copy()\nX_test_transformed[num_cols] = scaller.transform(X_test_transformed[num_cols])\n\ntest_df_transformed = test_df.copy()\ntest_df_transformed[num_cols] = scaller.transform(test_df_transformed[num_cols])\n\ny_train_log = np.log(y_train+1)\n\n\n# Выборка, включающая все спарсиные данные. Будем применять ее для обучения перед submission\nX_transformed = X.copy()\nX_transformed[num_cols] = scaller.transform(X_transformed[num_cols])\n\ny_log = np.log(y+1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Результаты будем сохранять в словаре results","metadata":{}},{"cell_type":"code","source":"results = {}\nversion = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Построим линейную регрессию с целой и логарифмированой целевой переменной\n\nКомментируем все модели, представив их результаты.","metadata":{}},{"cell_type":"code","source":"'''\nlr = LinearRegression()\nprint ('Исследуем линейную регрессию');\nversion = 1\n\nmape = learn_model(lr, X_transformed, X_test_transformed, y, y_test)\npredict_test, predict_submission = model_prediction(lr, X_test_transformed, test_df_transformed)\nsafe_results(results, 'LinearRegression', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n\n\nprint ('Исследуем линейную регрессию с логарифмированием целевой переменной');\nversion = 2\n\nmape = learn_model(lr, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(lr, X_test_transformed, test_df_transformed, True)\nsafe_results(results, 'LinearRegression_log', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 50.98%\n# kagle 226916\n\n# с логарифмированием целевой переменной\n# Средняя абсолютная ошибка в процентах: 15.24%\n# kagle 258490","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как и ожидалось, линейная регрессия абсолютно не справляется с задачей, всвязи с тем, что изменение цены нелинейно.","metadata":{}},{"cell_type":"markdown","source":"#### Обучим модель на стандартных настройках логистической регрессии.","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем логистическую регрессию');\nversion = 3\n\nlogreg = LogisticRegression(max_iter=100)\n\nmape = learn_model(logreg, X_transformed, X_test_transformed, y, y_test)\npredict_test, predict_submission = model_prediction(logreg, X_test_transformed, test_df_transformed)\nsafe_results(results, 'LogisticRegression', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 17.65%\n# kagle 36.53207","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Попробуем случайный лес","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем случайный лес');\nversion = 4\n\nrf = RandomForestRegressor(random_state = RANDOM_SEED, n_jobs = -1, verbose = 1)\n\nmape = learn_model(rf, X_transformed, X_test_transformed, y, y_test)\npredict_test, predict_submission = model_prediction(rf, X_test_transformed, test_df_transformed)\nsafe_results(results, 'LogisticRegression', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 3.08%\n# kagle 26.11943\n\n'''\nprint ('Исследуем случайный лес с логарифмированием целевой переменной');\nversion = 5\n\nmape = learn_model(rf, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(rf, X_test_transformed, test_df_transformed, True)\nsafe_results(results, 'LogisticRegression_log', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 2.93%\n# kagle 19.03580","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Подберем параметры для случайного леса","metadata":{}},{"cell_type":"code","source":"'''\nparameters = {'n_estimators': [200, 1100, 2000],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [10, 60, 110],\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]}\n\nrf = RandomForestRegressor(random_state=RANDOM_SEED)\nrandom_grid = RandomizedSearchCV(estimator=rf, param_distributions=parameters, n_iter=100, \n                               cv=3, verbose=2, random_state=RANDOM_SEED, n_jobs=-1)\n                               '''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nprint ('Исследуем случайный лес с гипер параметрами');\nversion = 6\n\nest = random_grid.best_estimator_\nrf_random = RandomForestRegressor(n_estimators=est.n_estimators, min_samples_split=2, min_samples_leaf=1, \n                             max_features=3, max_depth=25, bootstrap=True, random_state=RANDOM_SEED)\n\nmape = learn_model(rf_random, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(rf_random, X_test_transformed, test_df_transformed, True)\nsafe_results(results, 'RandomForestRegressor_HP', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ни одна из наших машин не смогла прогнать RandomizedSearchCV","metadata":{}},{"cell_type":"markdown","source":"#### Попробуем ExtraTreesRegressor","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем ExtraTreesRegressor с подбором гиперпараметров');\nversion = 7\nparameters = {'n_estimators' : [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)],\n              'max_depth' : [3, 5, 7, 10, 15, None],\n              'min_samples_split' : [2, 4, 6],\n              'bootstrap' : [True, False]\n             }\n\netr = ExtraTreesRegressor(random_state=RANDOM_SEED)\netr_grid = RandomizedSearchCV(estimator = etr,\n                        param_distributions = parameters, \n                        cv = 3, \n                        verbose=2)\nmape = learn_model(etr_grid, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(xgb_grid, X_test_transformed, test_df_transformed, True)\nsafe_results(results, 'ExtraTreesRegressorHP', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 2.84%\n# kagle - 19.9","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Бустинг","metadata":{}},{"cell_type":"markdown","source":"#### Попробуем градиентный бустинг ","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем GradientBoosting');\nversion = 10\n\ngb = GradientBoostingRegressor(min_samples_split=2, learning_rate=0.03, max_depth=10, n_estimators=300)\n\nmape = learn_model(gb, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(xb, X_transformed, test_df_transformed, True)\nsafe_results(results, 'GradientBoostingRegressor', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 7.26%\n# kagle 20.21","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Попробуем XGBRegressor с L1 regularization","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем XGBRegressor с alpha=1.5 (L1 regularization)');\nversion = 11\n\nxb = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.5, learning_rate=0.03, \\\n                      max_depth=12, reg_alpha=1.5, n_jobs=-1, n_estimators=500)\n\nmape = learn_model(xb, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(xb, X_transformed, test_df_transformed, True)\nsafe_results(results, 'XGBRegressorL1', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 7.96%\n# kagle 18.5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Заметно что дополнительные признаки оказывают положительное влияние","metadata":{}},{"cell_type":"markdown","source":"#### Попробуем XGBRegressor с L2 regularization","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем XGBRegressor с lambda=1.5 (L2 regularization)');\nversion = 12\n\nxb = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.5, learning_rate=0.03, \\\n                      max_depth=12, reg_lambda=1.5, n_jobs=-1, n_estimators=500)\nmape = learn_model(xb, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(xb, X_test_transformed, test_df_transformed, True)\nsafe_results(results, 'XGBRegressorL2', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 7.25%\n# kagle 18.81","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Попробуем CatBoostRegressor","metadata":{}},{"cell_type":"code","source":"'''\nfrom catboost import CatBoostRegressor\nprint ('Исследуем CatBoostRegressor')\nversion = 13\n\ncbr = CatBoostRegressor(iterations=5000, learning_rate=1, depth=2, random_seed=RANDOM_SEED)\n\nmape = learn_model(cbr, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(cbr, X_transformed, test_df_transformed, True)\nsafe_results(results, 'CatBoostRegressor', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 9.94%\n# kagle 28.33416","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Подберем параметры для CatBoostRegressor","metadata":{}},{"cell_type":"code","source":"# Уже подобранные значения ниже:\n\n# grid = {'learning_rate': [0.03, 0.1],\n#         'depth': [4, 6, 10],\n#         'l2_leaf_reg': [1, 3, 5, 7, 9]}\n# cbr = CatBoostRegressor(iterations=100, learning_rate=1, depth=2, random_seed=RANDOM_SEED)\n# grid_search_result = cbr.grid_search(grid, X=X_transformed, y=y_log, plot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Лучшими параметрами выбраны: {'depth': 10, 'l2_leaf_reg': 1, 'learning_rate': 0.1}","metadata":{}},{"cell_type":"code","source":"# grid_search_result\n# {'params': {'depth': 10, 'l2_leaf_reg': 1, 'learning_rate': 0.1}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Результат модели с лучшими параметрами","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем CatBoostRegressor с параметрами GridSearch')\nversion = 14\n\ncbr = CatBoostRegressor(iterations=2000,\n                        learning_rate=0.1,\n                        depth=10,\n                        l2_leaf_reg=1,\n                        random_seed=RANDOM_SEED)\n\nmape = learn_model(cbr, X_transformed, X_test_transformed, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(cbr, X_transformed, test_df_transformed, True)\nsafe_results(results, 'CatBoostRegressor_GS', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 7.99%\n# kagle 21.29833","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Примечение**\n\nДополнительно, чтобы улучшить качество моделей, мы пытались разделить все автомобили на 2 части (дорогие и дешевые). Для разделения использовали признак 'engine_power'. Обучали и предсказывали отдельно по каждому датасету. Результат соединяли в единый файл submission. Но в итоге значимого прироста качества не получили, поэтому далее представлен код для единого датасета","metadata":{}},{"cell_type":"code","source":"# cheap = full_df[full_df['engine_power'] <= 250]\n# expensive = full_df[full_df['engine_power'] > 250]\n# expensive.to_csv(path_or_buf=f'{TEST_DATA}expensive.csv')\n# cheap.to_csv(path_or_buf=f'{TEST_DATA}cheap.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Стекинг","metadata":{}},{"cell_type":"markdown","source":"#### Функции","metadata":{}},{"cell_type":"code","source":"# рассчёт алгоритма\ndef compute_meta_feature(regr, X_train, X_test, y_train, test_df, cv):\n    X_meta_train = np.zeros_like(y_train, dtype=np.float32)    \n\n    splits = cv.split(X_train)\n    print (splits)\n    for train_fold_index, predict_fold_index in splits:\n        X_fold_train, X_fold_predict = X_train[train_fold_index], X_train[predict_fold_index]\n        y_fold_train = y_train[train_fold_index]\n\n        folded_regr = clone(regr)\n        folded_regr.fit(X_fold_train, y_fold_train)\n\n        X_meta_train[predict_fold_index] = folded_regr.predict(X_fold_predict)\n\n    meta_regr = clone(regr)\n    meta_regr.fit(X_train, y_train)\n\n    X_meta_test = meta_regr.predict(X_test)\n    X_meta_pred = meta_regr.predict(test_df)\n\n    return X_meta_train, X_meta_test, X_meta_pred\n\n# рассчёт набора алгоритмов\ndef generate_meta_features(regr, X_train, X_test, y_train, test_df, cv):\n    features = [compute_meta_feature(regr, X_train, X_test, y_train, test_df, cv) for regr in tqdm(regr)]    \n    stacked_features_train = np.vstack([features_train for features_train, features_test, features_pred in features]).T\n    stacked_features_test = np.vstack([features_test for features_train, features_test, features_pred in features]).T\n    stacked_features_pred = np.vstack([features_pred for features_train, features_test, features_pred in features]).T\n    return stacked_features_train, stacked_features_test, stacked_features_pred\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Список моделей для стэкинга","metadata":{}},{"cell_type":"code","source":"# выбраем наиболее успешные модели\nmodels = [\nRandomForestRegressor(random_state = RANDOM_SEED, n_jobs = -1, verbose = 1),\nExtraTreesRegressor(n_estimators=300, \n                    criterion='mse', \n                    bootstrap=True, \n                    n_jobs=-1, \n                    random_state=RANDOM_SEED\n            ),\nXGBRegressor(objective='reg:squarederror', \n                    n_estimators=300,\n                    colsample_bytree=0.5, \n                    learning_rate=0.03,\n                    max_depth=12, \n                    reg_alpha=1.5, \n                    n_jobs=-1\n            ),\nXGBRegressor(objective='reg:squarederror', \n                    n_estimators=300,\n                    colsample_bytree=0.5, \n                    learning_rate=0.03,\n                    max_depth=12, \n                    reg_lambda=1.5, \n                    n_jobs=-1\n            )\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Проведем обучение базовых моделей","metadata":{}},{"cell_type":"code","source":"'''\ncv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\nstacked_features_train, stacked_features_test, stacked_features_pred = generate_meta_features(models\n    , X_transformed.values, X_test_transformed.values, y_log, test_df_transformed.values, cv)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Проведем обучение мета модели","metadata":{}},{"cell_type":"markdown","source":"Попробуем линейную регрессию в качестве мета модели","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем линейную регрессию');\nversion = 20\n\nlr = LinearRegression()\nmape = learn_model(lr, stacked_features_train, stacked_features_test, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(lr, stacked_features_test, stacked_features_pred, True)\nsafe_results(results, 'Stacking', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 4.65%\n# kagle 18.58","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Попробуем случайный лес в качестве мета модели","metadata":{}},{"cell_type":"code","source":"'''\nprint ('Исследуем случайный лес');\nversion = 21\n\nrf = RandomForestRegressor(random_state = RANDOM_SEED, n_jobs = -1, verbose = 1)\nmape = learn_model(rf, stacked_features_train, stacked_features_test, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(rf, stacked_features_test, stacked_features_pred, True)\nsafe_results(results, 'Stacking', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 5.23%\n# kagle 19.52","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Попробуем другой состав стекинга","metadata":{}},{"cell_type":"code","source":"'''\nmodels = [\n    RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1),\n    ExtraTreesRegressor(n_estimators=200, min_samples_split=2,\n                        max_depth=None, bootstrap=True)\n]\n\ncv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\nstacked_features_train, stacked_features_test, stacked_features_pred = generate_meta_features(\n    models, X_transformed.values, X_test_transformed.values, y_log, test_df_transformed.values, cv)\n\nprint('Исследуем линейную регрессию')\nversion = 22\n\nlr = LinearRegression()\nmape = learn_model(lr, stacked_features_train,\n                   stacked_features_test, y_log, y_test, True)\npredict_test, predict_submission = model_prediction(\n    lr, stacked_features_test, stacked_features_pred, True)\nsafe_results(results, 'Stacking', mape, predict_test, predict_submission)\nmake_submission(predict_submission, version)\n'''\n\n# Средняя абсолютная ошибка в процентах: 2.85%\n# kagle 18.77","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nmodels = [\nRandomForestRegressor(random_state = RANDOM_SEED, n_jobs = -1),\nExtraTreesRegressor(n_estimators=300, \n                    criterion='mse', \n                    bootstrap=True, \n                    n_jobs=-1, \n                    random_state=RANDOM_SEED\n            ),\nXGBRegressor(objective='reg:squarederror', \n                    n_estimators=300,\n                    colsample_bytree=0.5, \n                    learning_rate=0.03,\n                    max_depth=12, \n                    reg_alpha=1.5, \n                    n_jobs=-1\n            ),\nKNeighborsRegressor(algorithm = 'brute', weights = 'distance', p=1\n            )\n]\n\ncv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\nstacked_features_train, stacked_features_test, stacked_features_pred = generate_meta_features(models\n    , X_transformed.values, X_test_transformed.values, y_log, test_df_transformed.values, cv)\n'''\n\n# Средняя абсолютная ошибка в процентах: 6.44%\n# kagle 19.06","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Результат работы:","metadata":{}},{"cell_type":"markdown","source":"По результатам обучения различных моделей, наилучший результат показала XGBRegressor с параметром reg_lambda=1.5\n\nВ работе удалось создать модель, которая предсказывает стоимость автомобиля по его характеристикам. Эту модель можно использовать для выявления выгодных предложений, когда желаемая цена продавца ниже предсказанной рыночной цены.\n\nВ ходе создания модели было сделано:\n1. Составлен train датасет из данных с сайта auto.ru\n2. На этих данных обучена модель\n3. Дополнительно извлечены и сгенерированы новые признаки, в том числе с учетом изменения курса валюты\n4. Подобрана оптимальные параметры модели\n5. Попроботано большое количество разных алгоритмов и библиотек ML\n6. Реализован Stacking","metadata":{}},{"cell_type":"markdown","source":"## Выводы:","metadata":{}},{"cell_type":"markdown","source":"Сильное различие в метриках на обучаемом и тестовом датасете могут быть связаны с различным периодом парсинга данных.\n\nБольшинство моделей (исключая линейную регрессию) показали близкие результаты. Дальнейшее улучшение прогноза можно попытаться достичь за счет подбора гипер параметров.\n\nК сожалению, ни один из компьютеров не смог выполнить полный подбор гипер параметров за разумное время. В связи с этим заметно переобучение на обучаемом датасете.","metadata":{}}]}